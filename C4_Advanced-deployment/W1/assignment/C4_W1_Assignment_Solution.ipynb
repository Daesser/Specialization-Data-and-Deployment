{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-2-public/blob/adding_C4/C4/W1/assignment/C4_W1_Assignment_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zX4Kg8DUTKWO"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXdEnmVvXdUa"
   },
   "source": [
    "# Train Your Own Model and Serve It With TensorFlow Serving\n",
    "\n",
    "In this notebook, you will train a neural network to classify images of handwritten digits from the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. You will then save the trained model, and serve it using [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcLTAmF5Xs2T"
   },
   "source": [
    "**Warning: This notebook is designed to be run in a Google Colab only**.  It installs packages on the system and requires root access. If you want to run it in a local Jupyter notebook, please proceed with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2Q8bkjeYTl-"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "g8r89tTPI-Kb"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XGFJmWjrKttn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ Using TensorFlow Version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq-214o8SNt0"
   },
   "source": [
    "## Import the MNIST Dataset\n",
    "\n",
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset contains 70,000 grayscale images of the digits 0 through 9. The images show individual digits at a low resolution (28 by 28 pixels). \n",
    "\n",
    "Even though these are really images, we will load them as NumPy arrays and not as binary image objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AIT-qX0QzLo-"
   },
   "outputs": [],
   "source": [
    "# EXERCISE: Scale the values of the arrays below to be between 0.0 and 1.0.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIDGu-EEzdKb"
   },
   "source": [
    "In the cell below use the `.reshape` method to resize the arrays to the following sizes:\n",
    "\n",
    "```python\n",
    "train_images.shape: (60000, 28, 28, 1)\n",
    "test_images.shape: (10000, 28, 28, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XsIxeG6BzN4t"
   },
   "outputs": [],
   "source": [
    "# EXERCISE: Reshape the arrays below.\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aUw8ZxigB1Nx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_images.shape: (60000, 28, 28, 1), of float64\n",
      "test_images.shape: (10000, 28, 28, 1), of float64\n"
     ]
    }
   ],
   "source": [
    "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
    "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcR0OKbOSj0c"
   },
   "source": [
    "## Look at a Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VQMs4v_oSo9v"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 42\n",
    "\n",
    "plt.imshow(test_images[idx].reshape(28,28), cmap=plt.cm.binary)\n",
    "plt.title('True Label: {}'.format(test_labels[idx]), fontdict={'size': 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn_-9OsPYnDp"
   },
   "source": [
    "## Build a Model\n",
    "\n",
    "In the cell below build a `tf.keras.Sequential` model that can be used to classify the images of the MNIST dataset. Feel free to use the simplest possible CNN. Make sure your model has the correct `input_shape` and the correct number of output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EgMgJJynMbVY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 13, 13, 8)         80        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                13530     \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Create a model.\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3,\n",
    "                               strides=2, activation='relu', name='Conv1'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLzXnZT1YvS6"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "In the cell below configure your model for training using the `adam` optimizer, `sparse_categorical_crossentropy` as the loss, and `accuracy` for your metrics. Then train the model for the given number of epochs, using the `train_images` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LTNN0ANGgA36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3457 - accuracy: 0.9056\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1581 - accuracy: 0.9552\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1163 - accuracy: 0.9668\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0958 - accuracy: 0.9716\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0818 - accuracy: 0.9762: 0s - los\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Configure the model for training.\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# EXERCISE: Train the model.\n",
    "history = model.fit(train_images, train_labels, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er_vrDhf4qu5"
   },
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gMD387B93f2g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0839\n",
      "accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Evaluate the model on the test images.\n",
    "results_eval = model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "for metric, value in zip(model.metrics_names, results_eval):\n",
    "    print(metric + ': {:.3}'.format(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGfmT8M1Yx5y"
   },
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9uFDoDW_7HX6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Conv1_input with unsupported characters which will be renamed to conv1_input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\david\\AppData\\Local\\Temp\\1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\david\\AppData\\Local\\Temp\\1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "export_path = C:\\Users\\david\\AppData\\Local\\Temp\\1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = tempfile.gettempdir()\n",
    "\n",
    "version = 1\n",
    "\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "\n",
    "if os.path.isdir(export_path):\n",
    "    print('\\nAlready saved a model, cleaning up\\n')\n",
    "    !rm -r {export_path}\n",
    "\n",
    "model.save(export_path, save_format=\"tf\")\n",
    "\n",
    "print('\\nexport_path = {}'.format(export_path))\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KziE3e9tY-hH"
   },
   "source": [
    "## Examine Your Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU4GDF_aYtfQ"
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir {export_path} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsDTdBGHZAzo"
   },
   "source": [
    "## Add TensorFlow Serving Distribution URI as a Package Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWg9X2QHlbGS"
   },
   "outputs": [],
   "source": [
    "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
    "# You would instead do:\n",
    "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
    "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
    "\n",
    "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
    "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
    "!apt update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l5XkzqNZNBU"
   },
   "source": [
    "## Install TensorFlow Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygwa9AgRloYy"
   },
   "outputs": [],
   "source": [
    "!apt-get install tensorflow-model-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd_PobAKZWW8"
   },
   "source": [
    "## Run the TensorFlow Model Server\n",
    "\n",
    "You will now launch the TensorFlow model server with a bash script. In the cell below use the following parameters when running the TensorFlow model server:\n",
    "\n",
    "* `rest_api_port`: Use port `8501` for your requests.\n",
    "\n",
    "\n",
    "* `model_name`: Use `digits_model` as your model name. \n",
    "\n",
    "\n",
    "* `model_base_path`: Use the environment variable `MODEL_DIR` defined below as the base path to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUgp3vUdU5GS"
   },
   "outputs": [],
   "source": [
    "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJDhHNJVnaLN"
   },
   "outputs": [],
   "source": [
    "# EXERCISE: Fill in the missing code below.\n",
    "%%bash --bg \n",
    "nohup tensorflow_model_server \\\n",
    "  --rest_api_port=8501 \\\n",
    "  --model_name=digits_model \\\n",
    "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxbeiOCUUs2z"
   },
   "outputs": [],
   "source": [
    "!tail server.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mUrIWVRZdNu"
   },
   "source": [
    "## Create JSON Object with Test Images\n",
    "\n",
    "In the cell below construct a JSON object and use the first three images of the testing set (`test_images`) as your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dsD7KQG1m-R"
   },
   "outputs": [],
   "source": [
    "# EXERCISE: Create JSON Object\n",
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRdyPl4CZ5CU"
   },
   "source": [
    "## Make Inference Request\n",
    "\n",
    "In the cell below, send a predict request as a POST to the server's REST endpoint, and pass it your test data. You should ask the server to give you the latest version of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGvFyuIzW6n6"
   },
   "outputs": [],
   "source": [
    "# EXERCISE: Fill in the code below\n",
    "\n",
    "# if this cell fails execution because of an \"...Failed to establish a new connection...\" error,\n",
    "# try replacing in the link below 'localhost' with '127.0.0.1'\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://192.168.1.1:8501/v1/models/digits_model:predict', data=data, headers=headers)\n",
    "\n",
    "predictions = json.loads(json_response.text)['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtrFMts_ackX"
   },
   "source": [
    "## Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxQzj34aiDz1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(test_images[i].reshape(28,28), cmap = plt.cm.binary)\n",
    "    plt.axis('off')\n",
    "    color = 'green' if np.argmax(predictions[i]) == test_labels[i] else 'red'\n",
    "    plt.title('Prediction: {}\\nTrue Label: {}'.format(np.argmax(predictions[i]), test_labels[i]), color=color)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4vE_EKG2JjY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "C4_W1_Assignment_Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
